
<!doctype html>
<html lang="zh" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="Rust NB!">
      
      
        <meta name="author" content="Eurekaimer">
      
      
        <link rel="canonical" href="https://www.eurekaimer.xyz/Stathelper/%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/DMML_24Fall_Re/">
      
      
      
      
        
      
      
      <link rel="icon" href="../../../images/xiaoju.jpg">
      <meta name="generator" content="zensical-0.0.19">
    
    
      
        <title>DMML_24Fall_Re - Eurekaimer's Digital Garden</title>
      
    
    
      
        
      
      <link rel="stylesheet" href="../../../assets/stylesheets/modern/main.d4922b3c.min.css">
      
        
          
        
        <link rel="stylesheet" href="../../../assets/stylesheets/modern/palette.dfe2e883.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=LXGW+WenKai:300,300i,400,400i,500,500i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"LXGW WenKai";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../../../stylesheets/extra.css">
    
      <link rel="stylesheet" href="../../../stylesheets/anime-grid.css">
    
      <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lxgw-wenkai-webfont@1.1.0/style.css">
    
    <script>__md_scope=new URL("../../..",location),__md_hash=e=>[...e].reduce(((e,t)=>(e<<5)-e+t.charCodeAt(0)),0),__md_get=(e,t=localStorage,a=__md_scope)=>JSON.parse(t.getItem(a.pathname+"."+e)),__md_set=(e,t,a=localStorage,_=__md_scope)=>{try{a.setItem(_.pathname+"."+e,JSON.stringify(t))}catch(e){}},document.documentElement.setAttribute("data-platform",navigator.platform)</script>
    
      

    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="indigo">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#dmml_24fall_re" class="md-skip">
          跳转至
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow md-header--lifted" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="页眉">
    <a href="../../.." title="Eurekaimer&#x27;s Digital Garden" class="md-header__button md-logo" aria-label="Eurekaimer's Digital Garden" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" class="lucide lucide-pi" viewBox="0 0 24 24"><path d="M9 4v16M4 7c0-1.7 1.3-3 3-3h13"/><path d="M18 20c-1.7 0-3-1.3-3-3V4"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" class="lucide lucide-menu" viewBox="0 0 24 24"><path d="M4 5h16M4 12h16M4 19h16"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Eurekaimer's Digital Garden
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              DMML_24Fall_Re
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme)" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="indigo"  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" class="lucide lucide-sun-moon" viewBox="0 0 24 24"><path d="M12 2v2M14.837 16.385a6 6 0 1 1-7.223-7.222c.624-.147.97.66.715 1.248a4 4 0 0 0 5.26 5.259c.589-.255 1.396.09 1.248.715M16 12a4 4 0 0 0-4-4M19 5l-1.256 1.256M20 12h2"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme: light)" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="indigo"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_2" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" class="lucide lucide-sun" viewBox="0 0 24 24"><circle cx="12" cy="12" r="4"/><path d="M12 2v2M12 20v2M4.93 4.93l1.41 1.41M17.66 17.66l1.41 1.41M2 12h2M20 12h2M6.34 17.66l-1.41 1.41M19.07 4.93l-1.41 1.41"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme: dark)" data-md-color-scheme="slate" data-md-color-primary="indigo" data-md-color-accent="indigo"  aria-label="Switch to system preference"  type="radio" name="__palette" id="__palette_2">
    
      <label class="md-header__button md-icon" title="Switch to system preference" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" class="lucide lucide-moon" viewBox="0 0 24 24"><path d="M20.985 12.486a9 9 0 1 1-9.473-9.472c.405-.022.617.46.402.803a6 6 0 0 0 8.268 8.268c.344-.215.825-.004.803.401"/></svg>
      </label>
    
  
</form>
      
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" class="lucide lucide-search" viewBox="0 0 24 24"><path d="m21 21-4.34-4.34"/><circle cx="11" cy="11" r="8"/></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog" aria-label="查找">
  <button type="button" class="md-search__button">
    查找
  </button>
</div>
      
    
    <div class="md-header__source">
      
        <a href="https://github.com/Eurekaimer/Stathelper" title="前往仓库" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path fill="currentColor" d="M173.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M252.8 8C114.1 8 8 113.3 8 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C436.2 457.8 504 362.9 504 252 504 113.3 391.5 8 252.8 8M105.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
      
    </div>
  </nav>
  
    
      
<nav class="md-tabs" aria-label="标签" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="../../.." class="md-tabs__link">
        
  
  Stathelper

      </a>
    </li>
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../../HDP/" class="md-tabs__link">
          
  
  HDP

        </a>
      </li>
    
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../../STAT260/" class="md-tabs__link">
          
  
  STAT260

        </a>
      </li>
    
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../../CS61A/" class="md-tabs__link">
          
  
  CS61A

        </a>
      </li>
    
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../../Analysis/" class="md-tabs__link">
          
  
  Analysis

        </a>
      </li>
    
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../../ACG/Yuri/" class="md-tabs__link">
          
  
  ACG

        </a>
      </li>
    
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../../WebSource/" class="md-tabs__link">
          
  
  Web Source

        </a>
      </li>
    
  

      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="../../../friends-link/" class="md-tabs__link">
        
  
  Friends Link

      </a>
    </li>
  

      
    </ul>
  </div>
</nav>
    
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="导航栏" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../../.." title="Eurekaimer&#x27;s Digital Garden" class="md-nav__button md-logo" aria-label="Eurekaimer's Digital Garden" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" class="lucide lucide-pi" viewBox="0 0 24 24"><path d="M9 4v16M4 7c0-1.7 1.3-3 3-3h13"/><path d="M18 20c-1.7 0-3-1.3-3-3V4"/></svg>

    </a>
    Eurekaimer's Digital Garden
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/Eurekaimer/Stathelper" title="前往仓库" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path fill="currentColor" d="M173.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M252.8 8C114.1 8 8 113.3 8 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C436.2 457.8 504 362.9 504 252 504 113.3 391.5 8 252.8 8M105.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../.." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Stathelper
  

    
  </span>
  
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
          
        
      
        
      
    
    
    
      
      
        
      
    
    
      
        
        
      
    
    <li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
      
        
  
  
  
    <a href="../../../HDP/" class="md-nav__link">
      
  
  
  <span class="md-ellipsis">
    
  
    Index
  

    
  </span>
  
  

      
        <span class="md-nav__icon md-icon"></span>
      
    </a>
  

      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
          
        
      
        
      
    
    
    
      
      
        
      
    
    
      
        
        
      
    
    <li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
      
        
  
  
  
    <a href="../../../STAT260/" class="md-nav__link">
      
  
  
  <span class="md-ellipsis">
    
  
    STAT260 2021
  

    
  </span>
  
  

      
        <span class="md-nav__icon md-icon"></span>
      
    </a>
  

      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
          
        
      
        
      
        
      
        
      
    
    
    
      
      
        
      
    
    
      
        
        
      
    
    <li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
      
        
  
  
  
    <a href="../../../CS61A/" class="md-nav__link">
      
  
  
  <span class="md-ellipsis">
    
  
    CS61A
  

    
  </span>
  
  

      
        <span class="md-nav__icon md-icon"></span>
      
    </a>
  

      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
          
        
      
        
      
    
    
    
      
      
        
      
    
    
      
        
        
      
    
    <li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
      
        
  
  
  
    <a href="../../../Analysis/" class="md-nav__link">
      
  
  
  <span class="md-ellipsis">
    
  
    分析学总站
  

    
  </span>
  
  

      
        <span class="md-nav__icon md-icon"></span>
      
    </a>
  

      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
      
        
      
    
    
    
      
      
        
      
    
    
      
        
        
      
    
    <li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
      
        
  
  
  
    <a href="../../../ACG/Yuri/" class="md-nav__link">
      
  
  
  <span class="md-ellipsis">
    
  
    ACG
  

    
  </span>
  
  

      
        <span class="md-nav__icon md-icon"></span>
      
    </a>
  

      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
          
        
      
        
      
        
      
    
    
    
      
      
        
      
    
    
      
        
        
      
    
    <li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
      
        
  
  
  
    <a href="../../../WebSource/" class="md-nav__link">
      
  
  
  <span class="md-ellipsis">
    
  
    TBA
  

    
  </span>
  
  

      
        <span class="md-nav__icon md-icon"></span>
      
    </a>
  

      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../friends-link/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Friends Link
  

    
  </span>
  
  

      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


<nav class="md-nav md-nav--secondary" aria-label="目录">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      目录
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#lecture1-" class="md-nav__link">
    <span class="md-ellipsis">
      
        Lecture1 - 绪论 &amp; 基础复习
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Lecture1 - 绪论 &amp; 基础复习">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1" class="md-nav__link">
    <span class="md-ellipsis">
      
        1. 课程考核与大作业 (关键信息)
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2-data-mining-fundamentals" class="md-nav__link">
    <span class="md-ellipsis">
      
        2. 数据挖掘核心概念 (Data Mining Fundamentals)
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2. 数据挖掘核心概念 (Data Mining Fundamentals)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#21-technical-challenges" class="md-nav__link">
    <span class="md-ellipsis">
      
        2.1 新数据的挑战 (Technical Challenges)
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#22-taxonomy" class="md-nav__link">
    <span class="md-ellipsis">
      
        2.2 任务分类 (Taxonomy)
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2.2 任务分类 (Taxonomy)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#a-predictive-supervised" class="md-nav__link">
    <span class="md-ellipsis">
      
        A. 预测性任务 (Predictive) - Supervised
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#b-descriptive-unsupervised" class="md-nav__link">
    <span class="md-ellipsis">
      
        B. 描述性任务 (Descriptive) - Unsupervised
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3-process" class="md-nav__link">
    <span class="md-ellipsis">
      
        3. 机器学习系统构建流程 (Process)
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="3. 机器学习系统构建流程 (Process)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#31-abstraction-" class="md-nav__link">
    <span class="md-ellipsis">
      
        3.1 问题抽象化 (Abstraction) - 以“鱼类分类”为例
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#32-dataset-splitting-exam-point" class="md-nav__link">
    <span class="md-ellipsis">
      
        3.2 数据集划分 (Dataset Splitting) [Exam Point]
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#4-vs" class="md-nav__link">
    <span class="md-ellipsis">
      
        4. 监督 vs 无监督 (对比总结)
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#5-checklist" class="md-nav__link">
    <span class="md-ellipsis">
      
        5. 复习Checklist
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#lecture2-" class="md-nav__link">
    <span class="md-ellipsis">
      
        Lecture2 - 数据处理
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Lecture2 - 数据处理">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1-data-representation" class="md-nav__link">
    <span class="md-ellipsis">
      
        1. 数据的基本概念与表示 (Data Representation)
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="1. 数据的基本概念与表示 (Data Representation)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        数据定义与矩阵表示
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_2" class="md-nav__link">
    <span class="md-ellipsis">
      
        属性类型
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_3" class="md-nav__link">
    <span class="md-ellipsis">
      
        特殊数据类型的数学结构
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2-general-characteristics" class="md-nav__link">
    <span class="md-ellipsis">
      
        2. 数据集的一般特性 (General Characteristics)
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3-summary-statistics" class="md-nav__link">
    <span class="md-ellipsis">
      
        3. 汇总统计 (Summary Statistics)
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="3. 汇总统计 (Summary Statistics)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_4" class="md-nav__link">
    <span class="md-ellipsis">
      
        频率与众数
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#location-measures" class="md-nav__link">
    <span class="md-ellipsis">
      
        位置度量 (Location Measures)
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#dispersion-measures" class="md-nav__link">
    <span class="md-ellipsis">
      
        散布度量 (Dispersion Measures)
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#4-data-quality" class="md-nav__link">
    <span class="md-ellipsis">
      
        4. 数据质量与噪声 (Data Quality)
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#5-data-preprocessing" class="md-nav__link">
    <span class="md-ellipsis">
      
        5. 数据预处理 (Data Preprocessing)
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="5. 数据预处理 (Data Preprocessing)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#51" class="md-nav__link">
    <span class="md-ellipsis">
      
        5.1 聚集与抽样
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#52-discretization-binarization" class="md-nav__link">
    <span class="md-ellipsis">
      
        5.2 离散化与二元化 (Discretization &amp; Binarization)
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="5.2 离散化与二元化 (Discretization &amp; Binarization)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_5" class="md-nav__link">
    <span class="md-ellipsis">
      
        非监督离散化
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#entropy-based" class="md-nav__link">
    <span class="md-ellipsis">
      
        监督离散化：基于熵 (Entropy-based)
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#53-variable-transformation" class="md-nav__link">
    <span class="md-ellipsis">
      
        5.3 变量变换 (Variable Transformation)
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#6-feature-engineering" class="md-nav__link">
    <span class="md-ellipsis">
      
        6. 特征工程 (Feature Engineering)
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="6. 特征工程 (Feature Engineering)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_6" class="md-nav__link">
    <span class="md-ellipsis">
      
        特征创建与提取
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#dimensionality-reduction" class="md-nav__link">
    <span class="md-ellipsis">
      
        维归约 (Dimensionality Reduction)
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_7" class="md-nav__link">
    <span class="md-ellipsis">
      
        原始特征的问题
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#lecture3-dimensionality-reduction-concept-learning" class="md-nav__link">
    <span class="md-ellipsis">
      
        Lecture3 - 降维与概念学习 (Dimensionality Reduction &amp; Concept Learning)
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Lecture3 - 降维与概念学习 (Dimensionality Reduction &amp; Concept Learning)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        1. 降维技术概述
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2-feature-extraction" class="md-nav__link">
    <span class="md-ellipsis">
      
        2. 特征提取 (Feature Extraction)
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2. 特征提取 (Feature Extraction)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#21-pca-" class="md-nav__link">
    <span class="md-ellipsis">
      
        2.1 主成分分析 (PCA) - 无监督
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#22-lda-" class="md-nav__link">
    <span class="md-ellipsis">
      
        2.2 线性判别分析 (LDA) - 有监督
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3-feature-selection" class="md-nav__link">
    <span class="md-ellipsis">
      
        3. 特征选择 (Feature Selection)
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="3. 特征选择 (Feature Selection)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#31" class="md-nav__link">
    <span class="md-ellipsis">
      
        3.1 特征类型
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#32" class="md-nav__link">
    <span class="md-ellipsis">
      
        3.2 选择策略 (三大类)
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#4-concept-learning" class="md-nav__link">
    <span class="md-ellipsis">
      
        4. 概念学习 (Concept Learning)
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="4. 概念学习 (Concept Learning)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#41" class="md-nav__link">
    <span class="md-ellipsis">
      
        4.1 基础定义
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#42-partial-ordering" class="md-nav__link">
    <span class="md-ellipsis">
      
        4.2 偏序关系 (Partial Ordering)
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#43" class="md-nav__link">
    <span class="md-ellipsis">
      
        4.3 经典算法
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#5-inductive-bias-nfl" class="md-nav__link">
    <span class="md-ellipsis">
      
        5. 归纳偏置与理论 (Inductive Bias &amp; NFL)
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="5. 归纳偏置与理论 (Inductive Bias &amp; NFL)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#51-inductive-bias" class="md-nav__link">
    <span class="md-ellipsis">
      
        5.1 归纳偏置 (Inductive Bias)
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#52-nfl" class="md-nav__link">
    <span class="md-ellipsis">
      
        5.2 没有免费的午餐定理 (NFL)
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#lecture4-machine-learning-basics-image-features" class="md-nav__link">
    <span class="md-ellipsis">
      
        Lecture4 - 机器学习基础与图像特征 (Machine Learning Basics &amp; Image Features)
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Lecture4 - 机器学习基础与图像特征 (Machine Learning Basics &amp; Image Features)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1_2" class="md-nav__link">
    <span class="md-ellipsis">
      
        1. 机器学习的基本过程
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="1. 机器学习的基本过程">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#11" class="md-nav__link">
    <span class="md-ellipsis">
      
        1.1 一般流程
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#12" class="md-nav__link">
    <span class="md-ellipsis">
      
        1.2 举例
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2-image-features" class="md-nav__link">
    <span class="md-ellipsis">
      
        2. 图像特征基础 (Image Features)
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3" class="md-nav__link">
    <span class="md-ellipsis">
      
        3. 常见的图像特征描述子
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="3. 常见的图像特征描述子">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#31-color-features" class="md-nav__link">
    <span class="md-ellipsis">
      
        3.1 颜色特征 (Color Features)
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#32-shape-features" class="md-nav__link">
    <span class="md-ellipsis">
      
        3.2 形状特征 (Shape Features)
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#33-texture-features" class="md-nav__link">
    <span class="md-ellipsis">
      
        3.3 纹理特征 (Texture Features)
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#34-bag-of-words-bow" class="md-nav__link">
    <span class="md-ellipsis">
      
        3.4 中层语义特征：视觉词袋模型 (Bag-of-Words, BoW)
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#lecture5-model-assessment-and-selection" class="md-nav__link">
    <span class="md-ellipsis">
      
        Lecture5 - 模型评估与选择 (Model Assessment and Selection)
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Lecture5 - 模型评估与选择 (Model Assessment and Selection)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1-loss-risk" class="md-nav__link">
    <span class="md-ellipsis">
      
        1. 损失函数与风险 (Loss &amp; Risk)
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="1. 损失函数与风险 (Loss &amp; Risk)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#11-loss-function" class="md-nav__link">
    <span class="md-ellipsis">
      
        1.1 损失函数 (Loss Function)
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#12-risk-function" class="md-nav__link">
    <span class="md-ellipsis">
      
        1.2 风险函数 (Risk Function)
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#13" class="md-nav__link">
    <span class="md-ellipsis">
      
        1.3 模型选择策略
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2-performance-measures" class="md-nav__link">
    <span class="md-ellipsis">
      
        2. 性能度量 (Performance Measures)
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2. 性能度量 (Performance Measures)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#21" class="md-nav__link">
    <span class="md-ellipsis">
      
        2.1 分类任务的基础指标
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#22" class="md-nav__link">
    <span class="md-ellipsis">
      
        2.2 混淆矩阵与衍生指标
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#23-p-r-roc" class="md-nav__link">
    <span class="md-ellipsis">
      
        2.3 P-R 曲线与 ROC 曲线
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        3. 模型检验与比较方法
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="3. 模型检验与比较方法">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#31-evaluation-methods" class="md-nav__link">
    <span class="md-ellipsis">
      
        3.1 评估方法 (Evaluation Methods)
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#32-hypothesis-testing" class="md-nav__link">
    <span class="md-ellipsis">
      
        3.2 统计假设检验 (Hypothesis Testing)
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#4-bias-variance-decomposition" class="md-nav__link">
    <span class="md-ellipsis">
      
        4. 偏差-方差分解 (Bias-Variance Decomposition)
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#lecture6-linear-classifiers-and-optimization-criteria" class="md-nav__link">
    <span class="md-ellipsis">
      
        Lecture6 - 线性分类器与优化准则 (Linear Classifiers and Optimization Criteria)
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Lecture6 - 线性分类器与优化准则 (Linear Classifiers and Optimization Criteria)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1-linear-classifier-basics" class="md-nav__link">
    <span class="md-ellipsis">
      
        1. 线性分类器基础 (Linear Classifier Basics)
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="1. 线性分类器基础 (Linear Classifier Basics)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#11_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        1.1 基本概念
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#12-augmented-transformation" class="md-nav__link">
    <span class="md-ellipsis">
      
        1.2 增广变换 (Augmented Transformation)
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2-perpendicular-bisector-classifier" class="md-nav__link">
    <span class="md-ellipsis">
      
        2. 垂直平分分类器 (Perpendicular Bisector Classifier)
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2. 垂直平分分类器 (Perpendicular Bisector Classifier)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#21_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        2.1 设计思路
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#22_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        2.2 数学形式
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#23" class="md-nav__link">
    <span class="md-ellipsis">
      
        2.3 特点
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3-fisher-fisher-linear-discriminant" class="md-nav__link">
    <span class="md-ellipsis">
      
        3. Fisher 线性判别分析 (Fisher Linear Discriminant)
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="3. Fisher 线性判别分析 (Fisher Linear Discriminant)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#31_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        3.1 核心思想
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#32-fisher" class="md-nav__link">
    <span class="md-ellipsis">
      
        3.2 Fisher 准则函数
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#33" class="md-nav__link">
    <span class="md-ellipsis">
      
        3.3 最佳投影方向
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#4-perceptron-criterion" class="md-nav__link">
    <span class="md-ellipsis">
      
        4. 感知器准则 (Perceptron Criterion)
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="4. 感知器准则 (Perceptron Criterion)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#41_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        4.1 适用前提
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#42" class="md-nav__link">
    <span class="md-ellipsis">
      
        4.2 规范化
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#43_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        4.3 准则函数
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#44" class="md-nav__link">
    <span class="md-ellipsis">
      
        4.4 优化算法 (梯度下降)
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#lecture7-bayesian-classifiers" class="md-nav__link">
    <span class="md-ellipsis">
      
        Lecture7 - 贝叶斯分类器 (Bayesian Classifiers)
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Lecture7 - 贝叶斯分类器 (Bayesian Classifiers)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1_3" class="md-nav__link">
    <span class="md-ellipsis">
      
        1. 贝叶斯理论基础
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="1. 贝叶斯理论基础">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#11_2" class="md-nav__link">
    <span class="md-ellipsis">
      
        1.1 核心公式
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#12-vs" class="md-nav__link">
    <span class="md-ellipsis">
      
        1.2 频率派 vs 贝叶斯派
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2-bayes-minimum-error-rate" class="md-nav__link">
    <span class="md-ellipsis">
      
        2. 最小错误率 Bayes 决策 (Minimum Error Rate)
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2. 最小错误率 Bayes 决策 (Minimum Error Rate)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#21_2" class="md-nav__link">
    <span class="md-ellipsis">
      
        2.1 决策规则
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#22_2" class="md-nav__link">
    <span class="md-ellipsis">
      
        2.2 特点
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3-bayes-minimum-risk" class="md-nav__link">
    <span class="md-ellipsis">
      
        3. 最小风险 Bayes 决策 (Minimum Risk)
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="3. 最小风险 Bayes 决策 (Minimum Risk)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#31_2" class="md-nav__link">
    <span class="md-ellipsis">
      
        3.1 问题的提出
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#32_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        3.2 风险函数
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#33_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        3.3 决策规则
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#4-minimax-decision" class="md-nav__link">
    <span class="md-ellipsis">
      
        4. 最小最大决策 (Minimax Decision)
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="4. 最小最大决策 (Minimax Decision)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#41_2" class="md-nav__link">
    <span class="md-ellipsis">
      
        4.1 适用场景
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#42_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        4.2 核心思想
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#5-bayes" class="md-nav__link">
    <span class="md-ellipsis">
      
        5. 正态分布下的 Bayes 分类器
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="5. 正态分布下的 Bayes 分类器">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_8" class="md-nav__link">
    <span class="md-ellipsis">
      
        特殊情况下的决策面
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#lecture8-advanced-linear-classifiers" class="md-nav__link">
    <span class="md-ellipsis">
      
        Lecture8 - 线性分类器进阶 (Advanced Linear Classifiers)
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Lecture8 - 线性分类器进阶 (Advanced Linear Classifiers)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1-minimum-squared-error-criterion" class="md-nav__link">
    <span class="md-ellipsis">
      
        1. 最小错分样本数准则 (Minimum Squared Error Criterion)
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="1. 最小错分样本数准则 (Minimum Squared Error Criterion)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#11_3" class="md-nav__link">
    <span class="md-ellipsis">
      
        1.1 问题的提出
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#12_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        1.2 准则函数
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#13_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        1.3 求解方法
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2-minimum-squared-error-mse" class="md-nav__link">
    <span class="md-ellipsis">
      
        2. 最小平方误差准则 (Minimum Squared Error, MSE)
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2. 最小平方误差准则 (Minimum Squared Error, MSE)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#21_3" class="md-nav__link">
    <span class="md-ellipsis">
      
        2.1 核心思想
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#22_3" class="md-nav__link">
    <span class="md-ellipsis">
      
        2.2 准则函数
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#23_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        2.3 极值解
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3_2" class="md-nav__link">
    <span class="md-ellipsis">
      
        3. 总结与回顾
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="3. 总结与回顾">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#31_3" class="md-nav__link">
    <span class="md-ellipsis">
      
        3.1 线性分类器对比
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#32_2" class="md-nav__link">
    <span class="md-ellipsis">
      
        3.2 实验与应用
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              
                



  


              
              <article class="md-content__inner md-typeset">
                
                  
  
    <a href="https://github.com/Eurekaimer/Stathelper/edit/master/docs/数据科学/数据挖掘与机器学习/DMML_24Fall_Re.md" title="编辑此页" class="md-content__button md-icon" rel="edit">
      
      <svg xmlns="http://www.w3.org/2000/svg" fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" class="lucide lucide-file-pen" viewBox="0 0 24 24"><path d="M12.659 22H18a2 2 0 0 0 2-2V8a2.4 2.4 0 0 0-.706-1.706l-3.588-3.588A2.4 2.4 0 0 0 14 2H6a2 2 0 0 0-2 2v9.34"/><path d="M14 2v5a1 1 0 0 0 1 1h5M10.378 12.622a1 1 0 0 1 3 3.003L8.36 20.637a2 2 0 0 1-.854.506l-2.867.837a.5.5 0 0 1-.62-.62l.836-2.869a2 2 0 0 1 .506-.853z"/></svg>
    </a>
  
  
    
      
    
    <a href="https://github.com/Eurekaimer/Stathelper/raw/master/docs/数据科学/数据挖掘与机器学习/DMML_24Fall_Re.md" title="查看本页的源代码" class="md-content__button md-icon">
      
      <svg xmlns="http://www.w3.org/2000/svg" fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" class="lucide lucide-file-code-2" viewBox="0 0 24 24"><path d="M4 12.15V4a2 2 0 0 1 2-2h8a2.4 2.4 0 0 1 1.706.706l3.588 3.588A2.4 2.4 0 0 1 20 8v12a2 2 0 0 1-2 2h-3.35"/><path d="M14 2v5a1 1 0 0 0 1 1h5M5 16l-3 3 3 3M9 22l3-3-3-3"/></svg>
    </a>
  


<h1 id="dmml_24fall_re">DMML_24Fall_Re</h1>
<h2 id="lecture1-">Lecture1 - 绪论 &amp; 基础复习</h2>
<h3 id="1">1. 课程考核与大作业 (关键信息)</h3>
<ul class="task-list">
<li class="task-list-item"><label class="task-list-control"><input type="checkbox" disabled checked/><span class="task-list-indicator"></span></label> <strong>成绩构成</strong>: 闭卷考试 (60%) + 实验 (20%) + 大作业 (20%)</li>
<li class="task-list-item"><label class="task-list-control"><input type="checkbox" disabled checked/><span class="task-list-indicator"></span></label> <strong>大作业 (四选一, 15-18周展示)</strong>:<ol>
<li><strong>遥感图像飞机检测</strong>: 目标检测 (Object Detection).</li>
<li><strong>“福”字识别</strong>: 图像分类, 重点解决<strong>类别不平衡 (Class Imbalance)</strong>.</li>
<li><strong>台风预报</strong>: 序列预测/回归, 需处理时空数据.</li>
<li><strong>图像区域分割提取</strong>: 语义分割, 核心难点是<strong>保持空间相关性</strong>.</li>
</ol>
</li>
</ul>
<h3 id="2-data-mining-fundamentals">2. 数据挖掘核心概念 (Data Mining Fundamentals)</h3>
<h4 id="21-technical-challenges">2.1 新数据的挑战 (Technical Challenges)</h4>
<p>传统的统计分析面临新数据的四大挑战，这也是引入机器学习算法的动机：</p>
<ul>
<li><strong>可伸缩性 (Scalability)</strong>: 数据无法一次性放入内存，需要 Out-of-core 或分布式算法.</li>
<li><strong>高维性 (High Dimensionality)</strong>: 维度灾难 (Curse of Dimensionality), 数据稀疏.</li>
<li><strong>异构性 (Heterogeneous)</strong>: 混合属性 (非结构化文本, 图像, 时间序列).</li>
<li><strong>分布性 (Distributed)</strong>: 数据地理分布, 需解决隐私与通信代价.</li>
</ul>
<h4 id="22-taxonomy">2.2 任务分类 (Taxonomy)</h4>
<p>根据目标变量的存在与否及类型划分：</p>
<h5 id="a-predictive-supervised">A. 预测性任务 (Predictive) - Supervised</h5>
<p>利用训练数据学习映射函数 <span class="arithmatex">\(y=f(x|\theta)\)</span></p>
<ol>
<li><strong>分类 (Classification)</strong><ul>
<li><strong>输入</strong>: 样本 <span class="arithmatex">\(\mathbf{x} \in \mathbb{R}^n\)</span>, 标签 <span class="arithmatex">\(c\)</span> (离散).</li>
<li><strong>目标</strong>: 学习判别界面或概率分布.</li>
<li><strong>常用算法</strong>: Decision Tree, KNN, SVM, ANN, Naive Bayes.</li>
</ul>
</li>
<li><strong>回归 (Regression)</strong><ul>
<li><strong>输入</strong>: 样本 <span class="arithmatex">\(\mathbf{x}\)</span>, 标签 <span class="arithmatex">\(y\)</span> (连续).</li>
<li><strong>目标</strong>: 最小化预测误差.</li>
<li><strong>公式</strong>: <span class="arithmatex">\((w^*, b^*) = \arg\min_{w,b} \sum_{i=1}^{m} (f(x_i) - y_i)^2\)</span> (以线性回归MSE为例).</li>
</ul>
</li>
<li><strong>异常检测 (Anomaly Detection)</strong><ul>
<li><strong>目标</strong>: 识别显著偏离分布的 <span class="arithmatex">\(x\)</span>. 应用于欺诈检测、网络入侵.</li>
</ul>
</li>
</ol>
<h5 id="b-descriptive-unsupervised">B. 描述性任务 (Descriptive) - Unsupervised</h5>
<p>发现数据内在结构，无标签 <span class="arithmatex">\(y\)</span>.</p>
<ol>
<li><strong>聚类 (Clustering)</strong><ul>
<li><strong>目标</strong>: 最大化类间距离 (Inter-cluster), 最小化类内距离 (Intra-cluster).</li>
<li><strong>度量</strong>: 常用欧氏距离 (Euclidean Distance).</li>
</ul>
</li>
<li><strong>关联规则 (Association Rule)</strong><ul>
<li><strong>形式</strong>: <span class="arithmatex">\(A \rightarrow B\)</span> (蕴含关系).</li>
<li><strong>应用</strong>: 购物篮分析 (Market Basket Analysis).</li>
</ul>
</li>
<li><strong>序列模式 (Sequential Pattern)</strong><ul>
<li>关联规则 + <strong>时间维度</strong> (Time attribute).</li>
</ul>
</li>
</ol>
<h3 id="3-process">3. 机器学习系统构建流程 (Process)</h3>
<h4 id="31-abstraction-">3.1 问题抽象化 (Abstraction) - 以“鱼类分类”为例</h4>
<ul>
<li><strong>输入</strong>: 物理对象 (鱼).</li>
<li><strong>特征工程 (Feature Extraction)</strong>:<ul>
<li>物理特征 <span class="arithmatex">\(\rightarrow\)</span> 数值特征向量 <span class="arithmatex">\(\mathbf{x} = [x_1, x_2]^T\)</span> (如 <span class="arithmatex">\(x_1\)</span>=长度, <span class="arithmatex">\(x_2\)</span>=亮度).</li>
<li><strong>特征优选</strong>: 权衡特征数量 (维度) 与 计算复杂度/过拟合风险.</li>
</ul>
</li>
<li><strong>类别抽象</strong>: <span class="arithmatex">\(\omega_1\)</span> (鲑鱼), <span class="arithmatex">\(\omega_2\)</span> (鲈鱼).</li>
<li><strong>决策规则 (Decision Rule)</strong>:<ul>
<li>设定阈值 <span class="arithmatex">\(x_0\)</span> (Threshold).</li>
<li>Rule: If <span class="arithmatex">\(x &lt; x_0\)</span> then <span class="arithmatex">\(\omega_1\)</span>, else <span class="arithmatex">\(\omega_2\)</span>.</li>
<li>高维情况表现为特征空间中的<strong>超平面</strong>划分.</li>
</ul>
</li>
</ul>
<h4 id="32-dataset-splitting-exam-point">3.2 数据集划分 (Dataset Splitting) <strong>[Exam Point]</strong></h4>
<p>严谨的ML流程必须包含三部分：</p>
<ol>
<li><strong>训练集 (Training Set)</strong>: 用于拟合模型参数 (Parameters, e.g., 权重 <span class="arithmatex">\(w\)</span>).</li>
<li><strong>验证集 (Validation Set)</strong>:<ul>
<li><strong>用途</strong>: 调整超参数 (Hyper-parameters), 模型选择.</li>
<li><strong>注意</strong>: 在此阶段评估模型性能，决定是否停止训练.</li>
</ul>
</li>
<li><strong>测试集 (Test Set)</strong>:<ul>
<li><strong>用途</strong>: 评估最终模型的泛化能力 (Generalization).</li>
<li><strong>铁律</strong>: <strong>绝对不能在测试集上调参 (No Peeking)!</strong> 测试集仅用于最后一次的 "Unseen data" 模拟.</li>
</ul>
</li>
</ol>
<h3 id="4-vs">4. 监督 vs 无监督 (对比总结)</h3>
<table>
<thead>
<tr>
<th style="text-align: left;">维度</th>
<th style="text-align: left;">监督学习 (Supervised)</th>
<th style="text-align: left;">无监督学习 (Unsupervised)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"><strong>Data</strong></td>
<td style="text-align: left;"><span class="arithmatex">\((x, y)\)</span>, <span class="arithmatex">\(y\)</span> is label</td>
<td style="text-align: left;"><span class="arithmatex">\(x\)</span>, Just data</td>
</tr>
<tr>
<td style="text-align: left;"><strong>Goal</strong></td>
<td style="text-align: left;">Learn a function <span class="arithmatex">\(x \rightarrow y\)</span></td>
<td style="text-align: left;">Learn structure/distribution of data</td>
</tr>
<tr>
<td style="text-align: left;"><strong>Examples</strong></td>
<td style="text-align: left;">Classification, Regression, Object Detection</td>
<td style="text-align: left;">Clustering, Dim-Reduction, Generative Models</td>
</tr>
</tbody>
</table>
<h3 id="5-checklist">5. 复习Checklist</h3>
<ul class="task-list">
<li class="task-list-item"><label class="task-list-control"><input type="checkbox" disabled checked/><span class="task-list-indicator"></span></label> 理解 4V 特点对算法设计的影响 (内存/计算效率).</li>
<li class="task-list-item"><label class="task-list-control"><input type="checkbox" disabled checked/><span class="task-list-indicator"></span></label> 能够用数学语言描述分类 (<span class="arithmatex">\(c=f(x)\)</span>) 与 回归 (Loss Function) 的区别.</li>
<li class="task-list-item"><label class="task-list-control"><input type="checkbox" disabled checked/><span class="task-list-indicator"></span></label> 牢记 Train/Val/Test 的功能区别 (特别是验证集的作用).</li>
<li class="task-list-item"><label class="task-list-control"><input type="checkbox" disabled checked/><span class="task-list-indicator"></span></label> 熟悉大作业中提到的四类问题对应的 ML 任务类型 (检测/不平衡分类/时序/分割).</li>
</ul>
<hr />
<h2 id="lecture2-">Lecture2 - 数据处理</h2>
<h3 id="1-data-representation">1. 数据的基本概念与表示 (Data Representation)</h3>
<h4 id="_1">数据定义与矩阵表示</h4>
<p>数据挖掘是在大型数据存储库中自动发现有用信息的过程。从数学角度看，数据集通常表示为<strong>数据矩阵 (Data Matrix)</strong>。</p>
<ul>
<li><strong>对象 (Objects/Samples)</strong>: 行向量，表示实体（如记录、点、向量、模式）。</li>
<li><strong>属性 (Attributes/Features)</strong>: 列向量，表示对象的维度。</li>
<li>
<p><strong>矩阵表示</strong>:
    设数据集 <span class="arithmatex">\(D\)</span> 包含 <span class="arithmatex">\(m\)</span> 个对象，每个对象有 <span class="arithmatex">\(n\)</span> 个属性，则 <span class="arithmatex">\(D\)</span> 可表示为 <span class="arithmatex">\(m \times n\)</span> 矩阵：</p>
<div class="arithmatex">\[
D = \begin{pmatrix}
x_{11} &amp; x_{12} &amp; \cdots &amp; x_{1n} \\
x_{21} &amp; x_{22} &amp; \cdots &amp; x_{2n} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
x_{m1} &amp; x_{m2} &amp; \cdots &amp; x_{mn}
\end{pmatrix}
\]</div>
<p>其中第 <span class="arithmatex">\(i\)</span> 行代表第 <span class="arithmatex">\(i\)</span> 个对象，第 <span class="arithmatex">\(j\)</span> 列代表第 <span class="arithmatex">\(j\)</span> 个属性。</p>
</li>
</ul>
<h4 id="_2">属性类型</h4>
<ul>
<li><strong>离散 (Discrete)</strong>: 具有有限或无限可数个值。</li>
<li><strong>连续 (Continuous)</strong>: 取实数值。</li>
<li><strong>非对称属性 (Asymmetric Attributes)</strong>: 只有非零值才是重要的（常见于稀疏数据）。</li>
</ul>
<h4 id="_3">特殊数据类型的数学结构</h4>
<ol>
<li><strong>文档数据 (Document Data)</strong>:<ul>
<li>表示为<strong>词 (Term) 向量</strong>。</li>
<li>每个分量对应词在文档中出现的频率（TF）。</li>
<li>通常导致<strong>高维稀疏矩阵</strong>。</li>
</ul>
</li>
<li><strong>图数据 (Graph Data)</strong>: 包含对象间的结构关系（邻接矩阵）。</li>
<li><strong>时序数据 (Temporal/Sequence Data)</strong>: 数据间存在序关系 <span class="arithmatex">\(t_1, t_2, \dots, t_n\)</span>。</li>
<li><strong>图像数据</strong>:<ul>
<li>数学表示: <span class="arithmatex">\(f(x, y, \lambda, t)\)</span><ul>
<li><span class="arithmatex">\((x, y)\)</span>: 2-D 空间坐标。</li>
<li><span class="arithmatex">\(\lambda\)</span>: 波长（对应颜色/光谱）。</li>
<li><span class="arithmatex">\(t\)</span>: 时间（对应视频帧）。</li>
</ul>
</li>
<li>离散化后为像素矩阵，像素值代表亮度（灰度值）。</li>
</ul>
</li>
</ol>
<h3 id="2-general-characteristics">2. 数据集的一般特性 (General Characteristics)</h3>
<ul>
<li><strong>维度 (Dimensionality)</strong>: 属性的数量。<ul>
<li><strong>维数灾难 (Curse of Dimensionality)</strong>: 随着维度增加，数据在空间中变得极其稀疏，距离度量（如欧氏距离）失效，模型泛化能力下降。</li>
</ul>
</li>
<li><strong>稀疏性 (Sparsity)</strong>: 对象的大部分属性值为0。</li>
<li><strong>分辨率 (Resolution)</strong>: 数据的聚合程度，不同的分辨率可能表现出不同的统计模式。</li>
</ul>
<h3 id="3-summary-statistics">3. 汇总统计 (Summary Statistics)</h3>
<p>用于捕捉数据分布特征的度量。</p>
<h4 id="_4">频率与众数</h4>
<ul>
<li><strong>频率</strong>: 给定属性值 <span class="arithmatex">\(v_i\)</span>，其频率为具有该值的对象数除以总对象数 <span class="arithmatex">\(m\)</span>。</li>
<li><strong>众数 (Mode)</strong>: 频率最高的值。<ul>
<li><em>注</em>: 对连续属性，众数通常无意义（除非离散化），主要用于填充缺失值。</li>
</ul>
</li>
</ul>
<h4 id="location-measures">位置度量 (Location Measures)</h4>
<ul>
<li><strong>均值 (Mean)</strong>: <span class="arithmatex">\(\bar{x} = \frac{1}{m} \sum_{i=1}^{m} x_i\)</span><ul>
<li>缺点: 对离群点 (Outliers) 敏感。</li>
</ul>
</li>
<li>
<p><strong>中位数 (Median)</strong>:</p>
<div class="arithmatex">\[
\text{median}(x) = \begin{cases} 
x_{(r+1)} &amp; \text{若 } m \text{ 为奇数, } m=2r+1 \\
\frac{1}{2}(x_{(r)} + x_{(r+1)}) &amp; \text{若 } m \text{ 为偶数, } m=2r
\end{cases}
\]</div>
<ul>
<li>特点: 鲁棒性强 (Robust)。</li>
<li><strong>截断均值 (Trimmed Mean)</strong>: 去掉两端 <span class="arithmatex">\(p\%\)</span> 的极端值后计算的均值。</li>
</ul>
</li>
</ul>
<h4 id="dispersion-measures">散布度量 (Dispersion Measures)</h4>
<ul>
<li><strong>极差 (Range)</strong>: <span class="arithmatex">\(\text{range}(x) = \max(x) - \min(x)\)</span></li>
<li><strong>方差 (Variance)</strong>: <span class="arithmatex">\(s_x^2 = \frac{1}{m-1} \sum_{i=1}^{m} (x_i - \bar{x})^2\)</span></li>
<li><strong>绝对平均偏差 (AAD)</strong>: <span class="arithmatex">\(\text{AAD}(x) = \frac{1}{m} \sum_{i=1}^{m} |x_i - \bar{x}|\)</span></li>
<li><strong>中位数绝对偏差 (MAD)</strong>: <span class="arithmatex">\(\text{MAD}(x) = \text{median}(\{|x_1 - \bar{x}|, \dots, |x_m - \bar{x}|\})\)</span><ul>
<li><em>注</em>: MAD 对离群点非常鲁棒。</li>
</ul>
</li>
<li><strong>四分位数极差 (IQR)</strong>: <span class="arithmatex">\(IQR = x_{75\%} - x_{25\%}\)</span></li>
</ul>
<h3 id="4-data-quality">4. 数据质量与噪声 (Data Quality)</h3>
<ul>
<li><strong>噪声 (Noise)</strong>: 测量误差的<strong>随机</strong>部分。<ul>
<li>数学模型: <span class="arithmatex">\(Observed = Signal + Noise\)</span>。</li>
<li>处理: 信号处理、鲁棒算法。</li>
</ul>
</li>
<li><strong>离群点 (Outlier)</strong>: 特征显著不同于其他大部分数据的对象。<ul>
<li>可能是异常（Anomalous）对象，也可能是合法的高价值对象（如欺诈检测）。</li>
</ul>
</li>
<li><strong>缺失值处理策略</strong>:<ul>
<li>删除对象/属性。</li>
<li>估计/插值（均值、众数、回归）。</li>
<li>加权填补（用所有可能值代替，以可能性为权重）。</li>
</ul>
</li>
</ul>
<h3 id="5-data-preprocessing">5. 数据预处理 (Data Preprocessing)</h3>
<h4 id="51">5.1 聚集与抽样</h4>
<ul>
<li><strong>聚集 (Aggregation)</strong>: 转换标度（如从天到月），减少数据变异性 (Variability)，提高稳定性。</li>
<li><strong>抽样 (Sampling)</strong>: 选择子集进行分析。<ul>
<li><em>动机</em>: 克服计算资源限制（与统计学中“获取数据成本高”的动机不同，数据挖掘关注的是“处理全量数据成本高”）。</li>
</ul>
</li>
</ul>
<h4 id="52-discretization-binarization">5.2 离散化与二元化 (Discretization &amp; Binarization)</h4>
<p>将连续属性转换为分类属性，或将多分类转换为二元属性。</p>
<h5 id="_5">非监督离散化</h5>
<ul>
<li>等宽 (Equal Width)。</li>
<li>等频 (Equal Frequency)。</li>
<li>K-means 聚类离散化。</li>
</ul>
<h5 id="entropy-based">监督离散化：基于熵 (Entropy-based)</h5>
<p>利用类标号 (Class Labels) 信息，通过最小化熵来寻找最佳分割点。</p>
<ul>
<li>
<p><strong>区间熵的计算</strong>:
    设 <span class="arithmatex">\(k\)</span> 为类标号数，<span class="arithmatex">\(m_i\)</span> 是第 <span class="arithmatex">\(i\)</span> 个区间中值的总数，<span class="arithmatex">\(m_{ij}\)</span> 是第 <span class="arithmatex">\(i\)</span> 个区间中属于类 <span class="arithmatex">\(j\)</span> 的值的数量。
    第 <span class="arithmatex">\(i\)</span> 个区间的熵 <span class="arithmatex">\(e_i\)</span> 定义为：</p>
<div class="arithmatex">\[
e_i = - \sum_{j=1}^{k} p_{ij} \log_2 p_{ij}
\]</div>
<p>其中 <span class="arithmatex">\(p_{ij} = \frac{m_{ij}}{m_i}\)</span> 是第 <span class="arithmatex">\(i\)</span> 个区间中类 <span class="arithmatex">\(j\)</span> 的概率。</p>
</li>
<li>
<p><strong>总熵 (分割质量度量)</strong>:
    该分割的总熵 <span class="arithmatex">\(e\)</span> 是每个区间熵的加权平均：</p>
<div class="arithmatex">\[
e = \sum_{i=1}^{n} w_i e_i
\]</div>
<p>其中 <span class="arithmatex">\(w_i = \frac{m_i}{m}\)</span> 是第 <span class="arithmatex">\(i\)</span> 个区间样本占总样本的比例，<span class="arithmatex">\(n\)</span> 是区间个数。
<em>算法逻辑</em>: 寻找使总熵 <span class="arithmatex">\(e\)</span> 最小的分割点，递归进行。</p>
</li>
</ul>
<h4 id="53-variable-transformation">5.3 变量变换 (Variable Transformation)</h4>
<ul>
<li><strong>函数变换</strong>: <span class="arithmatex">\(\log(x), e^x, |x|\)</span> 等，用于改变数据分布。</li>
<li><strong>标准化/归一化 (Normalization)</strong>:<ul>
<li><strong>Z-score</strong>: <span class="arithmatex">\(x' = \frac{x - \bar{x}}{s}\)</span> (利用均值和标准差)。</li>
<li><strong>鲁棒标准化</strong>: 使用中位数代替均值，绝对标准差代替标准差。</li>
<li><em>目的</em>: 消除量纲影响，保持数值稳定性（对神经网络等基于梯度的算法至关重要）。</li>
</ul>
</li>
</ul>
<h3 id="6-feature-engineering">6. 特征工程 (Feature Engineering)</h3>
<h4 id="_6">特征创建与提取</h4>
<ul>
<li><strong>特征提取 (Feature Extraction)</strong>: 针对具体领域（如图像、音频），由原始数据构建新特征。</li>
<li><strong>特征映射</strong>: 将数据映射到新空间（如傅立叶变换检测周期性模式）。</li>
</ul>
<h4 id="dimensionality-reduction">维归约 (Dimensionality Reduction)</h4>
<ul>
<li><strong>动机</strong>:<ul>
<li>避免维数灾难。</li>
<li>降低噪声（删除不相关特征）。</li>
<li>降低时间/空间复杂度。</li>
<li>可视化需求。</li>
</ul>
</li>
<li><strong>常用方法</strong>:<ul>
<li><strong>PCA (主成分分析)</strong>: 线性、非监督，寻找最大方差方向。</li>
<li><strong>LDA (线性判别分析)</strong>: 线性、监督，寻找最大类间距离方向。</li>
</ul>
</li>
</ul>
<h4 id="_7">原始特征的问题</h4>
<ul>
<li><strong>相关性低</strong>: 特征与分类问题无关。</li>
<li><strong>病态矩阵</strong>: 特征过多且样本有限时，计算逆矩阵或参数估计时容易出现数值不稳定（病态矩阵问题）。</li>
</ul>
<h2 id="lecture3-dimensionality-reduction-concept-learning">Lecture3 - 降维与概念学习 (Dimensionality Reduction &amp; Concept Learning)</h2>
<h3 id="1_1">1. 降维技术概述</h3>
<p>数据预处理中的核心步骤，旨在解决<strong>维数灾难 (Curse of Dimensionality)</strong>。</p>
<ul>
<li><strong>目的</strong>:<ul>
<li>提高算法的时间和内存效率。</li>
<li>去除<strong>不相关特征</strong>与<strong>噪声</strong>，提升模型泛化能力。</li>
<li>数据可视化 (通常降至2维或3维)。</li>
</ul>
</li>
<li><strong>分类</strong>:<ol>
<li><strong>特征提取 (Feature Extraction)</strong>: 通过数学变换将原始特征空间映射到新的低维空间 (如 PCA, LDA)。</li>
<li><strong>特征选择 (Feature Selection)</strong>: 从原始特征集中选出一个最佳子集 (如 Filter, Wrapper, Embedded)。</li>
</ol>
</li>
</ul>
<h3 id="2-feature-extraction">2. 特征提取 (Feature Extraction)</h3>
<p>将 <span class="arithmatex">\(M\)</span> 个原始特征通过线性或非线性组合，转换为 <span class="arithmatex">\(m\)</span> 个新特征 (<span class="arithmatex">\(m &lt; M\)</span>)。</p>
<h4 id="21-pca-">2.1 主成分分析 (PCA) - 无监督</h4>
<ul>
<li><strong>核心思想</strong>: 将数据投影到方差最大的方向上。<ul>
<li><strong>方差 (Variance)</strong> 被视为信息的度量，方差越大，包含的信息（区分度）越大。</li>
</ul>
</li>
<li><strong>数学原理</strong>:<ul>
<li>寻找正交变换矩阵 <span class="arithmatex">\(A\)</span>，使得新特征 <span class="arithmatex">\(y = A^T x\)</span> 的协方差矩阵对角化。</li>
<li>选取<strong>协方差矩阵</strong>的前 <span class="arithmatex">\(m\)</span> 个最大特征值对应的特征向量作为主成分。</li>
</ul>
</li>
<li><strong>优缺点</strong>:<ul>
<li><em>优点</em>: 无参数限制，计算快，正交去相关。</li>
<li><em>缺点</em>: <strong>无监督</strong>（不利用类别标签），方差大的方向未必是分类效果最好的方向。</li>
</ul>
</li>
</ul>
<h4 id="22-lda-">2.2 线性判别分析 (LDA) - 有监督</h4>
<ul>
<li><strong>核心思想 (Fisher 投影准则)</strong>: 寻找最佳投影方向，使得投影后满足：<ul>
<li><strong>类内散度 (Intra-class scatter) 最小</strong>: 同类样本尽可能聚集。</li>
<li><strong>类间散度 (Inter-class scatter) 最大</strong>: 不同类样本尽可能分开。</li>
</ul>
</li>
<li>
<p><strong>优化目标</strong>:
    最大化广义瑞利商:</p>
<div class="arithmatex">\[ J(w) = \frac{w^T S_B w}{w^T S_W w} \]</div>
<p>其中 <span class="arithmatex">\(S_B\)</span> 为类间散度矩阵，<span class="arithmatex">\(S_W\)</span> 为类内散度矩阵。</p>
</li>
</ul>
<h3 id="3-feature-selection">3. 特征选择 (Feature Selection)</h3>
<p>直接筛选特征子集，保留特征的原始物理含义。</p>
<h4 id="31">3.1 特征类型</h4>
<ul>
<li><strong>冗余特征 (Redundant)</strong>: 信息被其他特征包含（如“购买价格”与“含税价格”）。</li>
<li><strong>不相关特征 (Irrelevant)</strong>: 对任务完全无用（如“学生ID”预测“成绩”）。</li>
</ul>
<h4 id="32">3.2 选择策略 (三大类)</h4>
<ol>
<li><strong>过滤法 (Filter)</strong><ul>
<li><strong>机制</strong>: 在学习算法运行前进行，独立于具体模型。</li>
<li><strong>Relief 算法</strong>: 设计“相关统计量”。<ul>
<li>计算样本与<strong>同类近邻 (Near-hit)</strong> 和<strong>异类近邻 (Near-miss)</strong> 的距离差。</li>
<li>如果特征能让同类更近、异类更远，则权重增加。</li>
</ul>
</li>
</ul>
</li>
<li><strong>包装法 (Wrapper)</strong><ul>
<li><strong>机制</strong>: 将学习算法作为“黑盒”，直接用模型的性能（如准确率）来评价特征子集。</li>
<li><strong>LVW (Las Vegas Wrapper)</strong>: 随机采样特征子集 <span class="arithmatex">\(\rightarrow\)</span> 交叉验证评估 <span class="arithmatex">\(\rightarrow\)</span> 寻找最优。</li>
<li><em>特点</em>: 效果通常比过滤法好，但计算开销极大。</li>
</ul>
</li>
<li><strong>嵌入法 (Embedded)</strong><ul>
<li><strong>机制</strong>: 特征选择过程与学习器训练过程融为一体。</li>
<li><strong>L1 正则化 (LASSO)</strong>:<ul>
<li>优化目标: <span class="arithmatex">\(\min \sum (y_i - w^T x_i)^2 + \lambda \|w\|_1\)</span></li>
<li><strong>几何解释</strong>: L1范数 (<span class="arithmatex">\(|w_1| + |w_2| \le C\)</span>) 的等值线是方形，与损失函数的切点容易落在坐标轴上，导致部分系数 <span class="arithmatex">\(w_i\)</span> 变为 0，从而实现<strong>稀疏化</strong>（特征选择）。</li>
<li><em>对比</em>: L2范数（圆形）通常只让权重变小，不易减为0。</li>
</ul>
</li>
</ul>
</li>
</ol>
<hr />
<h3 id="4-concept-learning">4. 概念学习 (Concept Learning)</h3>
<h4 id="41">4.1 基础定义</h4>
<ul>
<li><strong>定义</strong>: 从训练样例中逼近一个布尔函数（目标概念）的过程。</li>
<li><strong>术语</strong>:<ul>
<li><strong>实例集合 (X)</strong>: 总体。</li>
<li><strong>目标概念 (c)</strong>: <span class="arithmatex">\(c: X \rightarrow \{0, 1\}\)</span> (待学习的真理)。</li>
<li><strong>训练样例 (D)</strong>: 样本对 <span class="arithmatex">\(\langle x, c(x) \rangle\)</span>。</li>
<li><strong>假设 (h)</strong>: 学习器给出的估计函数，<span class="arithmatex">\(h \in H\)</span> (假设空间)。</li>
</ul>
</li>
</ul>
<h4 id="42-partial-ordering">4.2 偏序关系 (Partial Ordering)</h4>
<p>假设空间存在“一般”与“特殊”的层级结构：
*   <strong>More General (<span class="arithmatex">\(\ge_g\)</span>)</strong>: 如果假设 <span class="arithmatex">\(h_j\)</span> 覆盖的实例集合包含 <span class="arithmatex">\(h_k\)</span> 覆盖的集合，则称 <span class="arithmatex">\(h_j\)</span> 比 <span class="arithmatex">\(h_k\)</span> 更一般。
*   <em>作用</em>: 使得我们可以通过搜索（而非枚举）来寻找目标假设。</p>
<h4 id="43">4.3 经典算法</h4>
<ol>
<li>
<p><strong>Find-S 算法</strong></p>
<ul>
<li><strong>策略</strong>: 寻找与正例一致的<strong>极大特殊 (Maximally Specific)</strong> 假设。</li>
<li><em>流程</em>: 初始化 <span class="arithmatex">\(h\)</span> 为全 <span class="arithmatex">\(\emptyset\)</span> (最特殊)。每遇到一个<strong>正例</strong>，将 <span class="arithmatex">\(h\)</span> 中不匹配的属性约束放宽（泛化）以覆盖该正例。</li>
<li><em>缺点</em>: 忽略反例，无法判断是否存在多个一致假设，对噪声敏感。</li>
</ul>
</li>
<li>
<p><strong>候选消除算法 (Candidate-Elimination)</strong></p>
<ul>
<li><strong>变型空间 (Version Space)</strong>: 假设空间 <span class="arithmatex">\(H\)</span> 中所有与训练集 <span class="arithmatex">\(D\)</span> <strong>一致</strong>的假设集合。</li>
<li><strong>边界表示</strong>:<ul>
<li><strong>S集</strong>: 极大特殊边界 (Specific Boundary)。</li>
<li><strong>G集</strong>: 极大一般边界 (General Boundary)。</li>
<li>变型空间即为 <span class="arithmatex">\(S\)</span> 与 <span class="arithmatex">\(G\)</span> 之间的所有假设。</li>
</ul>
</li>
<li><em>流程</em>:<ul>
<li>正例 <span class="arithmatex">\(\rightarrow\)</span> 泛化 <span class="arithmatex">\(S\)</span> (使其更一般)，移除 <span class="arithmatex">\(G\)</span> 中不一致的。</li>
<li>反例 <span class="arithmatex">\(\rightarrow\)</span> 特殊化 <span class="arithmatex">\(G\)</span> (使其更特殊)，移除 <span class="arithmatex">\(S\)</span> 中不一致的。</li>
</ul>
</li>
<li><em>收敛</em>: 当 <span class="arithmatex">\(S\)</span> 和 <span class="arithmatex">\(G\)</span> 收敛到同一个假设时，找到目标概念。</li>
</ul>
</li>
</ol>
<h3 id="5-inductive-bias-nfl">5. 归纳偏置与理论 (Inductive Bias &amp; NFL)</h3>
<h4 id="51-inductive-bias">5.1 归纳偏置 (Inductive Bias)</h4>
<ul>
<li><strong>定义</strong>: 学习算法在学习过程中对某种类型假设的偏好或预先假定。</li>
<li><strong>重要性</strong>: 如果没有归纳偏置，算法无法对<strong>未见样本 (Unseen Instances)</strong> 进行分类（即无法进行归纳推理）。</li>
<li><strong>类型</strong>:<ul>
<li><strong>奥卡姆剃刀 (Occam's Razor)</strong>: 偏好简单的假设（若多个假设与观察一致，选最简单的）。</li>
<li><strong>限制偏置</strong>: 限制假设空间的形式（如只允许合取式）。</li>
</ul>
</li>
</ul>
<h4 id="52-nfl">5.2 没有免费的午餐定理 (NFL)</h4>
<ul>
<li><strong>内容</strong>: 如果对所有可能的问题（所有可能的分布）求平均，所有学习算法（包括随机猜测）的期望性能是相同的。</li>
<li>
<div class="arithmatex">\[ \sum_f E_{ote}(\mathcal{L}_a | X, f) = \sum_f E_{ote}(\mathcal{L}_b | X, f) \]</div>
</li>
<li><strong>启示</strong>:<ul>
<li>不存在“万能”的最佳算法。</li>
<li>算法的优劣取决于其<strong>归纳偏置</strong>是否与<strong>具体问题</strong>的特征相匹配。</li>
<li>研究机器学习必须关注具体的问题背景。</li>
</ul>
</li>
</ul>
<h2 id="lecture4-machine-learning-basics-image-features">Lecture4 - 机器学习基础与图像特征 (Machine Learning Basics &amp; Image Features)</h2>
<h3 id="1_2">1. 机器学习的基本过程</h3>
<p>机器学习的本质是<strong>寻找一个函数 (Looking for a Function)</strong>，通过数据拟合输入与输出之间的映射关系。</p>
<h4 id="11">1.1 一般流程</h4>
<ol>
<li><strong>特征提取 (Feature Extraction)</strong>: 将原始数据（如图像像素、音频波形）转化为计算机可理解的特征向量。</li>
<li><strong>定义模型集合 (Define a Set of Functions)</strong>: 选定一个假设空间（如线性模型、神经网络）。</li>
<li><strong>确定最优准则 (Goodness of Function)</strong>: 定义损失函数（Loss Function）来评估模型的好坏。</li>
<li><strong>选择最优映射 (Pick the Best Function)</strong>: 利用学习算法（如梯度下降）在假设空间中搜索最优解。</li>
</ol>
<h4 id="12">1.2 举例</h4>
<ul>
<li><strong>图像识别</strong>: <span class="arithmatex">\(f(\text{Image}) = \text{"Cat"}\)</span></li>
<li><strong>语音识别</strong>: <span class="arithmatex">\(f(\text{Audio}) = \text{"How are you"}\)</span></li>
<li><strong>围棋</strong>: <span class="arithmatex">\(f(\text{Board State}) = \text{Next Move}\)</span></li>
</ul>
<h3 id="2-image-features">2. 图像特征基础 (Image Features)</h3>
<p>特征是连接原始数据与机器学习算法的桥梁。根据人类视觉系统的感知，图像特征分为三个层次：</p>
<ol>
<li><strong>低层特征 (Low-level Features)</strong>: 描述图像的视觉属性（外观、边缘、纹理、形状）。</li>
<li><strong>中层语义 (Mid-level Representations)</strong>: 连接低层与高层的纽带，如视觉词袋 (BoW)。</li>
<li><strong>高层语义特征 (High-level Semantic Features)</strong>: 场景、行为、情感等抽象概念。</li>
</ol>
<hr />
<h3 id="3">3. 常见的图像特征描述子</h3>
<h4 id="31-color-features">3.1 颜色特征 (Color Features)</h4>
<ul>
<li><strong>颜色矩 (Color Moments)</strong>:<ul>
<li>利用一阶矩（均值 <span class="arithmatex">\(\mu\)</span>）、二阶矩（标准差 <span class="arithmatex">\(\sigma\)</span>）、三阶矩（偏度 <span class="arithmatex">\(s\)</span>）来描述颜色分布。</li>
<li>通常在 RGB 空间计算，共 9 个分量（3个通道 <span class="arithmatex">\(\times\)</span> 3个矩）。</li>
</ul>
</li>
<li><strong>颜色直方图 (Color Histogram)</strong>:<ul>
<li>统计图像中不同颜色出现的频率。</li>
<li>特点: 对图像旋转、平移、缩放具有不变性，但丢失了空间信息。</li>
</ul>
</li>
</ul>
<h4 id="32-shape-features">3.2 形状特征 (Shape Features)</h4>
<ul>
<li><strong>Hu 不变矩 (Hu Moments)</strong>:<ul>
<li>利用二阶和三阶中心矩构造出的 7 个不变矩（<span class="arithmatex">\(\phi_1 \sim \phi_7\)</span>）。</li>
<li><strong>特点</strong>: 对平移、旋转、尺度缩放具有不变性 (Translation, Rotation, Scale Invariant)。</li>
<li>常用于简单的物体识别（如商标、二值图像）。</li>
</ul>
</li>
<li><strong>傅里叶描述子 (Fourier Descriptors)</strong>:<ul>
<li>将物体边界视为封闭曲线，通过傅里叶变换将边界坐标转换为频域系数。</li>
<li>低频系数描述整体形状，高频系数描述细节。</li>
</ul>
</li>
<li><strong>Hough 变换 (Hough Transform)</strong>:<ul>
<li>用于检测直线、圆等参数化形状。</li>
<li>核心思想: 将图像空间的点映射到参数空间（投票机制）。</li>
<li>直线检测: 利用极坐标方程 <span class="arithmatex">\(\rho = x \cos\theta + y \sin\theta\)</span>。</li>
</ul>
</li>
</ul>
<h4 id="33-texture-features">3.3 纹理特征 (Texture Features)</h4>
<ul>
<li><strong>灰度共生矩阵 (GLCM)</strong>:<ul>
<li>统计像素对 <span class="arithmatex">\((i, j)\)</span> 在特定方向 <span class="arithmatex">\(\theta\)</span> 和距离 <span class="arithmatex">\(d\)</span> 下同时出现的概率。</li>
<li>基于 GLCM 可计算能量、熵、对比度、相关性等统计量。</li>
</ul>
</li>
<li><strong>局部二值模式 (LBP)</strong>:<ul>
<li>比较中心像素与 <span class="arithmatex">\(3 \times 3\)</span> 邻域像素的大小，生成二进制编码。</li>
<li><strong>特点</strong>: 计算简单，对光照变化具有较强的鲁棒性，常用于人脸识别和纹理分类。</li>
</ul>
</li>
</ul>
<h4 id="34-bag-of-words-bow">3.4 中层语义特征：视觉词袋模型 (Bag-of-Words, BoW)</h4>
<ul>
<li><strong>灵感</strong>: 源于文本处理（统计文档中关键词的频率）。</li>
<li><strong>流程</strong>:<ol>
<li><strong>特征提取</strong>: 提取图像局部特征（如 SIFT）。</li>
<li><strong>生成码书 (Codebook Generation)</strong>: 利用 K-means 聚类将特征量化为“视觉单词 (Visual Words)”。</li>
<li><strong>特征编码</strong>: 统计图像中每个视觉单词出现的频率，生成直方图向量。</li>
</ol>
</li>
<li><strong>应用</strong>: 图像检索、场景分类。</li>
</ul>
<h2 id="lecture5-model-assessment-and-selection">Lecture5 - 模型评估与选择 (Model Assessment and Selection)</h2>
<h3 id="1-loss-risk">1. 损失函数与风险 (Loss &amp; Risk)</h3>
<h4 id="11-loss-function">1.1 损失函数 (Loss Function)</h4>
<p>损失函数 <span class="arithmatex">\(L(y, f(x))\)</span> 用于度量模型预测值 <span class="arithmatex">\(f(x)\)</span> 与真实值 <span class="arithmatex">\(y\)</span> 之间的差异。常见的损失函数包括：
*   <strong>0-1 损失 (0-1 loss)</strong>: 用于分类问题，预测错误为1，正确为0。
*   <strong>平方损失 (Quadratic loss)</strong>: <span class="arithmatex">\(L(y, f(x)) = (y - f(x))^2\)</span>，常用于回归。
*   <strong>绝对损失 (Absolute loss)</strong>: <span class="arithmatex">\(L(y, f(x)) = |y - f(x)|\)</span>。
*   <strong>对数损失 (Logarithmic loss)</strong>: <span class="arithmatex">\(L(y, P(y|x)) = -\log P(y|x)\)</span>，常用于概率预测。</p>
<h4 id="12-risk-function">1.2 风险函数 (Risk Function)</h4>
<ul>
<li><strong>期望风险 (Expected Risk / Risk Function)</strong>: <span class="arithmatex">\(R_{exp}(f)\)</span> 是模型在联合分布 <span class="arithmatex">\(P(X,Y)\)</span> 下的平均损失，代表模型在<strong>总体</strong>样本上的表现（理论值，通常未知）。</li>
<li>
<p><strong>经验风险 (Empirical Risk)</strong>: <span class="arithmatex">\(R_{emp}(f)\)</span> 是模型在<strong>训练数据集</strong>上的平均损失。</p>
<div class="arithmatex">\[ R_{emp}(f) = \frac{1}{N} \sum_{i=1}^{N} L(y_i, f(x_i)) \]</div>
</li>
</ul>
<h4 id="13">1.3 模型选择策略</h4>
<ul>
<li><strong>经验风险最小化 (ERM)</strong>: 直接最小化训练误差。当样本量较小时，易导致<strong>过拟合 (Overfitting)</strong>。</li>
<li><strong>结构风险最小化 (SRM)</strong>: 在经验风险的基础上加入正则化项 <span class="arithmatex">\(\lambda J(f)\)</span>，平衡模型的拟合能力与复杂度，防止过拟合。</li>
</ul>
<h3 id="2-performance-measures">2. 性能度量 (Performance Measures)</h3>
<h4 id="21">2.1 分类任务的基础指标</h4>
<ul>
<li><strong>错误率 (Error Rate)</strong>: 误分类样本占比。</li>
<li><strong>准确率 (Accuracy)</strong>: 正确分类样本占比。<ul>
<li><em>局限性</em>: 在<strong>样本不平衡</strong>（如正样本极少）的情况下，准确率会失效（例如全部预测为负类也能得到极高准确率）。</li>
</ul>
</li>
</ul>
<h4 id="22">2.2 混淆矩阵与衍生指标</h4>
<p>混淆矩阵 (Confusion Matrix) 将预测结果分为四类：TP (真反), FN (假负), FP (假正), TN (真负)。</p>
<ul>
<li><strong>查准率/精度 (Precision)</strong>: <span class="arithmatex">\(P = \frac{TP}{TP + FP}\)</span>。预测为正的样本中有多少是真的正样本。</li>
<li><strong>查全率/召回率 (Recall/Sensitivity)</strong>: <span class="arithmatex">\(R = \frac{TP}{TP + FN}\)</span>。所有正样本中有多少被预测出来了。</li>
<li>
<p><strong>F1 度量</strong>: Precision 和 Recall 的调和平均数，用于综合评估。</p>
<div class="arithmatex">\[ F_1 = \frac{2 \times P \times R}{P + R} \]</div>
</li>
</ul>
<h4 id="23-p-r-roc">2.3 P-R 曲线与 ROC 曲线</h4>
<ul>
<li><strong>P-R 曲线</strong>: 以查准率 P 为纵轴，查全率 R 为横轴。</li>
<li><strong>ROC 曲线 (Receiver Operating Characteristic)</strong>:<ul>
<li>纵轴: <strong>真正率 (TPR)</strong> = Recall = <span class="arithmatex">\(\frac{TP}{TP + FN}\)</span></li>
<li>横轴: <strong>假正率 (FPR)</strong> = <span class="arithmatex">\(\frac{FP}{TN + FP}\)</span> (负样本中被误判为正的比例)</li>
<li><em>优势</em>: 相比 P-R 曲线，ROC 曲线对样本类别比例不敏感。</li>
</ul>
</li>
<li><strong>AUC (Area Under ROC Curve)</strong>: ROC 曲线下的面积。AUC 越大，模型性能越好（理想值为1，随机猜测为0.5）。</li>
</ul>
<h3 id="3_1">3. 模型检验与比较方法</h3>
<h4 id="31-evaluation-methods">3.1 评估方法 (Evaluation Methods)</h4>
<ul>
<li><strong>留出法 (Hold-out)</strong>: 将数据集划分为互斥的训练集和验证集（如 2/3 训练，1/3 验证）。需保持数据分布一致性，通常采用多次随机划分取平均。</li>
<li><strong>交叉验证法 (Cross Validation)</strong>:<ul>
<li><strong>K折交叉验证</strong>: 将数据分 K 份，轮流用 K-1 份训练，1 份测试。</li>
<li><strong>留一法 (LOO)</strong>: K 等于样本数，每次只留一个样本做测试。准确但计算量大。</li>
</ul>
</li>
<li><strong>自助法 (Bootstrapping)</strong>: 有放回采样，适合小数据集。</li>
</ul>
<h4 id="32-hypothesis-testing">3.2 统计假设检验 (Hypothesis Testing)</h4>
<p>用于判断两个模型性能的差异是否具有<strong>统计学意义</strong>，而非仅仅是随机波动。
*   <strong>二项检验 / t 检验</strong>: 比较单个或两个模型在特定数据集上的表现。
*   <strong>McNemar 检验</strong>: 比较两个分类器在同一测试集上的分类结果差异。
*   <strong>Friedman 检验 &amp; Nemenyi 检验</strong>: 用于<strong>多个算法</strong>在<strong>多个数据集</strong>上的性能比较与排序。</p>
<h3 id="4-bias-variance-decomposition">4. 偏差-方差分解 (Bias-Variance Decomposition)</h3>
<p>泛化误差可以分解为三部分：
$$ E(f; D) = \text{Bias}^2 + \text{Variance} + \text{Noise} $$</p>
<ul>
<li><strong>偏差 (Bias)</strong>: 学习算法的期望预测与真实结果的偏离程度。度量了算法的<strong>拟合能力</strong>。<ul>
<li><em>欠拟合 (Underfitting)</em>: 偏差主导，模型太简单，连训练集都学不好。</li>
</ul>
</li>
<li><strong>方差 (Variance)</strong>: 同样大小的训练集的变动所导致的学习性能的变化。度量了算法的<strong>稳定性</strong>。<ul>
<li><em>过拟合 (Overfitting)</em>: 方差主导，模型太复杂，对训练数据的微小扰动极其敏感。</li>
</ul>
</li>
<li><strong>噪声 (Noise)</strong>: 数据本身的固有误差，是学习性能的上限（无法克服）。</li>
</ul>
<p><strong>权衡 (Trade-off)</strong>:
*   <strong>增加模型复杂度</strong>（如增加多项式阶数）：偏差减小，方差增大（易过拟合）。
*   <strong>降低模型复杂度</strong>（如正则化）：偏差增大，方差减小（易欠拟合）。</p>
<h2 id="lecture6-linear-classifiers-and-optimization-criteria">Lecture6 - 线性分类器与优化准则 (Linear Classifiers and Optimization Criteria)</h2>
<h3 id="1-linear-classifier-basics">1. 线性分类器基础 (Linear Classifier Basics)</h3>
<h4 id="11_1">1.1 基本概念</h4>
<ul>
<li><strong>定义</strong>: 通过一个线性判别函数（直线、平面或超平面）将特征空间一分为二的分类器。</li>
<li><strong>判别函数</strong>:
    $$ g(x) = w^T x + w_0 $$
    其中 <span class="arithmatex">\(w\)</span> 为权向量，<span class="arithmatex">\(w_0\)</span> 为阈值（或偏置）。</li>
<li><strong>决策规则</strong>:<ul>
<li>若 <span class="arithmatex">\(g(x) &gt; 0\)</span>，判为正类 (<span class="arithmatex">\(\omega_1\)</span>)。</li>
<li>若 <span class="arithmatex">\(g(x) &lt; 0\)</span>，判为负类 (<span class="arithmatex">\(\omega_2\)</span>)。</li>
<li>若 <span class="arithmatex">\(g(x) = 0\)</span>，位于决策面上（边界）。</li>
</ul>
</li>
</ul>
<h4 id="12-augmented-transformation">1.2 增广变换 (Augmented Transformation)</h4>
<p>为了简化数学处理，将权向量 <span class="arithmatex">\(w\)</span> 和偏置 <span class="arithmatex">\(w_0\)</span> 合并：
*   <strong>增广样本向量</strong>: <span class="arithmatex">\(y = [1, x_1, x_2, \dots, x_d]^T\)</span> (维数 <span class="arithmatex">\(D+1\)</span>)。
*   <strong>增广权向量</strong>: <span class="arithmatex">\(a = [w_0, w_1, w_2, \dots, w_d]^T\)</span>。
*   <strong>新判别函数</strong>: <span class="arithmatex">\(g(x) = a^T y\)</span>。决策面变为过原点的超平面 <span class="arithmatex">\(a^T y = 0\)</span>。</p>
<h3 id="2-perpendicular-bisector-classifier">2. 垂直平分分类器 (Perpendicular Bisector Classifier)</h3>
<p>又称<strong>最小距离分类器 (Minimum Distance Classifier)</strong>。</p>
<h4 id="21_1">2.1 设计思路</h4>
<ul>
<li>基于两类样本的<strong>均值向量</strong> (<span class="arithmatex">\(m_1, m_2\)</span>)。</li>
<li>决策面是连接两类均值点线段的<strong>垂直平分面</strong>。</li>
</ul>
<h4 id="22_1">2.2 数学形式</h4>
<ul>
<li>
<p><strong>判别函数</strong>:</p>
<div class="arithmatex">\[ g(x) = (m_1 - m_2)^T x - \frac{1}{2}(m_1 - m_2)^T (m_1 + m_2) \]</div>
</li>
<li>
<p><strong>等价形式</strong>:</p>
<div class="arithmatex">\[ d(x, m_1) &lt; d(x, m_2) \Rightarrow x \in \omega_1 \]</div>
<p>即样本距离哪个类的均值更近，就判为哪一类。</p>
</li>
</ul>
<h4 id="23">2.3 特点</h4>
<ul>
<li>计算简单，无须复杂的优化过程。</li>
<li><strong>非最佳决策</strong>: 仅考虑均值，未考虑样本的分布方差（离散度）。</li>
</ul>
<h3 id="3-fisher-fisher-linear-discriminant">3. Fisher 线性判别分析 (Fisher Linear Discriminant)</h3>
<h4 id="31_1">3.1 核心思想</h4>
<p>将高维样本<strong>投影</strong>到一维直线上，使得投影后的样本：
*   <strong>类内距离最小</strong> (Within-class scatter minimized): 同类样本尽可能聚集。
*   <strong>类间距离最大</strong> (Between-class scatter maximized): 不同类样本尽可能分开。</p>
<h4 id="32-fisher">3.2 Fisher 准则函数</h4>
<div class="arithmatex">\[ J_F(w) = \frac{( \tilde{m}_1 - \tilde{m}_2 )^2}{\tilde{S}_1^2 + \tilde{S}_2^2} \]</div>
<p>其中 <span class="arithmatex">\(\tilde{m}\)</span> 为投影后的均值，<span class="arithmatex">\(\tilde{S}^2\)</span> 为投影后的类内离散度。</p>
<h4 id="33">3.3 最佳投影方向</h4>
<p>通过对 <span class="arithmatex">\(J_F(w)\)</span> 求导并令其为 0 (使用 Lagrange 乘子法)，得到最佳投影方向 <span class="arithmatex">\(w^*\)</span>：</p>
<div class="arithmatex">\[ w^* = S_W^{-1} (m_1 - m_2) \]</div>
<p>其中 <span class="arithmatex">\(S_W\)</span> 为总类内离散度矩阵。</p>
<h3 id="4-perceptron-criterion">4. 感知器准则 (Perceptron Criterion)</h3>
<h4 id="41_1">4.1 适用前提</h4>
<p>样本集必须是<strong>线性可分 (Linearly Separable)</strong> 的，即存在一个超平面能将两类完全分开。</p>
<h4 id="42">4.2 规范化</h4>
<p>为了统一处理，将所有负类 (<span class="arithmatex">\(\omega_2\)</span>) 样本的增广向量乘以 -1。
*   <strong>规范化后目标</strong>: 寻找权向量 <span class="arithmatex">\(a\)</span>，使得对所有样本 <span class="arithmatex">\(y_i\)</span>，都有 <span class="arithmatex">\(a^T y_i &gt; 0\)</span>。</p>
<h4 id="43_1">4.3 准则函数</h4>
<p>感知器准则函数只关注<strong>被错误分类</strong>的样本集合 <span class="arithmatex">\(Y_{error}\)</span>：</p>
<div class="arithmatex">\[ J_p(a) = \sum_{y \in Y_{error}} (-a^T y) \]</div>
<ul>
<li>若所有样本正确分类，<span class="arithmatex">\(Y_{error}\)</span> 为空，<span class="arithmatex">\(J_p(a) = 0\)</span> (极小值)。</li>
<li>若有错分，错分样本的 <span class="arithmatex">\(a^T y &lt; 0\)</span>，则 <span class="arithmatex">\(-a^T y &gt; 0\)</span>，导致 <span class="arithmatex">\(J_p(a) &gt; 0\)</span>。</li>
</ul>
<h4 id="44">4.4 优化算法 (梯度下降)</h4>
<p>使用梯度下降法迭代求解 <span class="arithmatex">\(a\)</span>：</p>
<div class="arithmatex">\[ a(k+1) = a(k) + \rho_k \sum_{y \in Y_{error}} y \]</div>
<ul>
<li><strong>物理意义</strong>: 当出现错分样本时，用该样本向量去“修正”权向量，直到所有样本都被正确分类。</li>
<li><strong>收敛性</strong>: 若样本线性可分，感知器算法保证收敛。</li>
</ul>
<h2 id="lecture7-bayesian-classifiers">Lecture7 - 贝叶斯分类器 (Bayesian Classifiers)</h2>
<h3 id="1_3">1. 贝叶斯理论基础</h3>
<h4 id="11_2">1.1 核心公式</h4>
<ul>
<li>
<p><strong>贝叶斯公式 (Bayes' Theorem)</strong>:</p>
<div class="arithmatex">\[ P(\omega_i | x) = \frac{P(x | \omega_i) P(\omega_i)}{P(x)} \]</div>
<ul>
<li><span class="arithmatex">\(P(\omega_i)\)</span>: <strong>先验概率 (Prior)</strong>，类别 <span class="arithmatex">\(\omega_i\)</span> 在观察到数据前发生的概率。</li>
<li><span class="arithmatex">\(P(x | \omega_i)\)</span>: <strong>类条件概率 (Class-Conditional Probability) / 似然 (Likelihood)</strong>，在类别 <span class="arithmatex">\(\omega_i\)</span> 下观察到特征 <span class="arithmatex">\(x\)</span> 的概率。</li>
<li><span class="arithmatex">\(P(\omega_i | x)\)</span>: <strong>后验概率 (Posterior)</strong>，在观察到特征 <span class="arithmatex">\(x\)</span> 后，样本属于类别 <span class="arithmatex">\(\omega_i\)</span> 的概率。</li>
<li><span class="arithmatex">\(P(x)\)</span>: <strong>证据因子 (Evidence)</strong>，归一化常数，<span class="arithmatex">\(P(x) = \sum_j P(x | \omega_j) P(\omega_j)\)</span>。</li>
</ul>
</li>
</ul>
<h4 id="12-vs">1.2 频率派 vs 贝叶斯派</h4>
<ul>
<li><strong>频率派 (Frequentist)</strong>: 认为参数 <span class="arithmatex">\(\theta\)</span> 是<strong>固定但未知</strong>的常数，通过最大化似然函数 <span class="arithmatex">\(P(X; \theta)\)</span> 来估计参数 (MLE)。</li>
<li><strong>贝叶斯派 (Bayesian)</strong>: 认为参数 <span class="arithmatex">\(\theta\)</span> 是<strong>随机变量</strong>，服从某个先验分布，通过最大化后验概率 <span class="arithmatex">\(P(\theta | X)\)</span> 来估计参数 (MAP)。</li>
</ul>
<h3 id="2-bayes-minimum-error-rate">2. 最小错误率 Bayes 决策 (Minimum Error Rate)</h3>
<h4 id="21_2">2.1 决策规则</h4>
<p>目标是使分类错误的概率最小化。
*   <strong>规则</strong>: 将样本 <span class="arithmatex">\(x\)</span> 分配给<strong>后验概率最大</strong>的类别。</p>
<pre><code>$$ \text{Decide } \omega_i \text{ if } P(\omega_i | x) &gt; P(\omega_j | x), \forall j \neq i $$
</code></pre>
<ul>
<li><strong>等价规则</strong>:<ul>
<li>比较分子 (忽略 <span class="arithmatex">\(P(x)\)</span>): <span class="arithmatex">\(p(x | \omega_i) P(\omega_i) &gt; p(x | \omega_j) P(\omega_j)\)</span></li>
<li>似然比检验: <span class="arithmatex">\(\frac{p(x | \omega_i)}{p(x | \omega_j)} &gt; \frac{P(\omega_j)}{P(\omega_i)}\)</span></li>
<li>对数似然比: <span class="arithmatex">\(\ln p(x | \omega_i) + \ln P(\omega_i) &gt; \ln p(x | \omega_j) + \ln P(\omega_j)\)</span></li>
</ul>
</li>
</ul>
<h4 id="22_2">2.2 特点</h4>
<ul>
<li>在概率意义上是最优的。</li>
<li>需要知道先验概率和类条件概率密度（通常难以准确获得）。</li>
</ul>
<h3 id="3-bayes-minimum-risk">3. 最小风险 Bayes 决策 (Minimum Risk)</h3>
<h4 id="31_2">3.1 问题的提出</h4>
<p>最小错误率假设所有错误的代价是一样的。但在实际应用中（如癌症诊断），<strong>漏报 (False Negative)</strong> 的代价通常远高于 <strong>误报 (False Positive)</strong>。</p>
<h4 id="32_1">3.2 风险函数</h4>
<ul>
<li><strong>损失函数 <span class="arithmatex">\(\lambda(\alpha_i, \omega_j)\)</span></strong>: 真实类别为 <span class="arithmatex">\(\omega_j\)</span> 但决策为 <span class="arithmatex">\(\alpha_i\)</span> 时的代价。</li>
<li><strong>条件风险 <span class="arithmatex">\(R(\alpha_i | x)\)</span></strong>: 对于特定样本 <span class="arithmatex">\(x\)</span>，采取决策 <span class="arithmatex">\(\alpha_i\)</span> 的期望损失。
    $$ R(\alpha_i | x) = \sum_{j=1}^C \lambda(\alpha_i, \omega_j) P(\omega_j | x) $$</li>
</ul>
<h4 id="33_1">3.3 决策规则</h4>
<ul>
<li>
<p><strong>规则</strong>: 选择使<strong>条件风险最小</strong>的决策。</p>
<div class="arithmatex">\[ \alpha^* = \arg \min_{\alpha_i} R(\alpha_i | x) \]</div>
</li>
<li>
<p><strong>与最小错误率的关系</strong>: 当采用 <strong>0-1 损失函数</strong> (对错分惩罚为1，正确为0) 时，最小风险决策等价于最小错误率决策。</p>
</li>
</ul>
<h3 id="4-minimax-decision">4. 最小最大决策 (Minimax Decision)</h3>
<h4 id="41_2">4.1 适用场景</h4>
<p>当先验概率 <span class="arithmatex">\(P(\omega_i)\)</span> <strong>未知或不确定</strong>时，传统的贝叶斯决策无法直接使用。</p>
<h4 id="42_1">4.2 核心思想</h4>
<ul>
<li>设计一个分类器，使得在<strong>最坏的先验概率分布</strong>下，风险也是最小的。</li>
<li>博弈论视角：假设“大自然”会选择一个让分类器风险最大的先验分布，而分类器的目标是最小化这个最大风险。</li>
<li>这是一种<strong>保守</strong>的策略，保证了性能的下限。</li>
</ul>
<h3 id="5-bayes">5. 正态分布下的 Bayes 分类器</h3>
<p>当类条件概率密度 <span class="arithmatex">\(p(x | \omega_i)\)</span> 服从多维正态分布 <span class="arithmatex">\(N(\mu_i, \Sigma_i)\)</span> 时，判别函数可以写成二次型形式：</p>
<div class="arithmatex">\[ g_i(x) = -\frac{1}{2}(x - \mu_i)^T \Sigma_i^{-1} (x - \mu_i) - \frac{1}{2} \ln |\Sigma_i| + \ln P(\omega_i) \]</div>
<h4 id="_8">特殊情况下的决策面</h4>
<ol>
<li><strong><span class="arithmatex">\(\Sigma_i = \sigma^2 I\)</span> (各维度独立等方差)</strong>: 决策面是<strong>线性</strong>的，退化为最小欧氏距离分类器（考虑先验修正）。</li>
<li><strong><span class="arithmatex">\(\Sigma_i = \Sigma\)</span> (各类协方差矩阵相同)</strong>: 决策面是<strong>线性</strong>的 (Linear Discriminant Analysis, LDA)。</li>
<li><strong><span class="arithmatex">\(\Sigma_i\)</span> 任意</strong>: 决策面是<strong>二次曲面</strong> (Quadratic Discriminant Analysis, QDA)，如双曲线、椭圆、抛物线等。</li>
</ol>
<h2 id="lecture8-advanced-linear-classifiers">Lecture8 - 线性分类器进阶 (Advanced Linear Classifiers)</h2>
<h3 id="1-minimum-squared-error-criterion">1. 最小错分样本数准则 (Minimum Squared Error Criterion)</h3>
<h4 id="11_3">1.1 问题的提出</h4>
<ul>
<li>感知器准则要求样本集必须是<strong>线性可分</strong>的，否则无法收敛。</li>
<li>在实际应用中，完全线性可分的情况很少见。</li>
<li><strong>目标</strong>: 寻找一个权向量，使得被错误分类的样本数量最少。</li>
</ul>
<h4 id="12_1">1.2 准则函数</h4>
<ul>
<li>
<p><strong>准则一</strong>: 直接最小化错分样本的模长。</p>
<div class="arithmatex">\[ J(a) = || Ya - b ||^2 \]</div>
<ul>
<li><span class="arithmatex">\(Y\)</span>: 样本矩阵。</li>
<li><span class="arithmatex">\(b\)</span>: 正常数向量（如全1向量）。</li>
<li><strong>准则二</strong>: 利用不等式约束。</li>
</ul>
<div class="arithmatex">\[ \max J(a) = \sum_{i=1}^N \frac{1 + \text{sgn}(a^T y_i)}{2} \]</div>
<ul>
<li><span class="arithmatex">\(\text{sgn}(u) = 1\)</span> 若 <span class="arithmatex">\(u \ge 0\)</span>，否则为 -1。</li>
<li>该函数直接统计正确分类的样本数，但难以优化（非连续、不可导）。</li>
</ul>
</li>
</ul>
<h4 id="13_1">1.3 求解方法</h4>
<ul>
<li>通常采用启发式搜索或近似算法。</li>
<li><strong>特点</strong>: 设计过程复杂，计算量大。</li>
</ul>
<h3 id="2-minimum-squared-error-mse">2. 最小平方误差准则 (Minimum Squared Error, MSE)</h3>
<h4 id="21_3">2.1 核心思想</h4>
<p>将分类问题转化为<strong>回归问题</strong>。</p>
<ul>
<li>将不等式约束 <span class="arithmatex">\(a^T y_i &gt; 0\)</span> 转化为等式约束 <span class="arithmatex">\(a^T y_i = b_i\)</span> (<span class="arithmatex">\(b_i &gt; 0\)</span>)。</li>
<li>目标是使实际输出 <span class="arithmatex">\(a^T y_i\)</span> 与期望输出 <span class="arithmatex">\(b_i\)</span> 之间的<strong>均方误差最小</strong>。</li>
</ul>
<h4 id="22_3">2.2 准则函数</h4>
<div class="arithmatex">\[ J(a) = || Ya - b ||^2 = \sum_{i=1}^N (a^T y_i - b_i)^2 \]</div>
<p>其中 <span class="arithmatex">\(Y\)</span> 是样本矩阵（每一行为一个样本），<span class="arithmatex">\(b\)</span> 是目标向量（通常设为全1）。</p>
<h4 id="23_1">2.3 极值解</h4>
<p>通过对 <span class="arithmatex">\(J(a)\)</span> 求导并令其为 0，可得解析解：</p>
<div class="arithmatex">\[ a^* = (Y^T Y)^{-1} Y^T b \]</div>
<ul>
<li><span class="arithmatex">\(Y^\dagger = (Y^T Y)^{-1} Y^T\)</span> 被称为 <span class="arithmatex">\(Y\)</span> 的<strong>伪逆矩阵 (Pseudo-Inverse)</strong>。</li>
<li><strong>特点</strong>:<ul>
<li>解析解存在且唯一（若 <span class="arithmatex">\(Y^T Y\)</span> 可逆）。</li>
<li>计算简单，无需迭代。</li>
<li>即使样本线性不可分，也能得到一个解（MSE 意义下的最优解）。</li>
</ul>
</li>
</ul>
<hr />
<h3 id="3_2">3. 总结与回顾</h3>
<h4 id="31_3">3.1 线性分类器对比</h4>
<table>
<thead>
<tr>
<th style="text-align: left;">准则</th>
<th style="text-align: left;">适用条件</th>
<th style="text-align: left;">优点</th>
<th style="text-align: left;">缺点</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"><strong>垂直平分</strong></td>
<td style="text-align: left;">无</td>
<td style="text-align: left;">简单直观</td>
<td style="text-align: left;">仅考虑均值，未考虑分布，非最优</td>
</tr>
<tr>
<td style="text-align: left;"><strong>Fisher</strong></td>
<td style="text-align: left;">无</td>
<td style="text-align: left;">考虑了类内离散度，类间距离最大化</td>
<td style="text-align: left;">需计算逆矩阵，投影后可能重叠</td>
</tr>
<tr>
<td style="text-align: left;"><strong>感知器</strong></td>
<td style="text-align: left;"><strong>线性可分</strong></td>
<td style="text-align: left;">保证收敛到解</td>
<td style="text-align: left;">不可分时不收敛，解不唯一</td>
</tr>
<tr>
<td style="text-align: left;"><strong>MSE</strong></td>
<td style="text-align: left;">无</td>
<td style="text-align: left;">有解析解，计算快</td>
<td style="text-align: left;">对离群点敏感，非分类错误率最小</td>
</tr>
</tbody>
</table>
<h4 id="32_2">3.2 实验与应用</h4>
<ul>
<li><strong>实验1</strong>: 手写LBP特征提取。LBP (Local Binary Pattern) 是一种纹理特征，对光照变化鲁棒。</li>
<li><strong>实验2</strong>: 垂直平分分类器编程实现。</li>
</ul>
<p><strong>Final Note</strong>: 线性分类器是机器学习的基石。虽然现代深度学习模型（如 CNN, Transformer）在复杂任务上表现更好，但线性模型因其可解释性强、计算效率高，在很多场景下仍然是首选基准模型。</p>














                
              </article>
            </div>
          
          
  <script>var tabs=__md_get("__tabs");if(Array.isArray(tabs))e:for(var set of document.querySelectorAll(".tabbed-set")){var labels=set.querySelector(".tabbed-labels");for(var tab of tabs)for(var label of labels.getElementsByTagName("label"))if(label.innerText.trim()===tab){var input=document.getElementById(label.htmlFor);input.checked=!0;continue e}}</script>

<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" class="lucide lucide-circle-arrow-up" viewBox="0 0 24 24"><circle cx="12" cy="12" r="10"/><path d="m16 12-4-4-4 4M12 16V8"/></svg>
  回到页面顶部
</button>
        
      </main>
      
        <footer class="md-footer">
  
    
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
    <div class="md-copyright__highlight">
      Copyright &copy; 2024-2026 Eurekaimer

    </div>
  
  
    Made with
    <a href="https://zensical.org/" target="_blank" rel="noopener">
      Zensical
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
      <div class="md-progress" data-md-component="progress" role="progressbar"></div>
    
    
    
      
      
      
      <script id="__config" type="application/json">{"annotate":null,"base":"../../..","features":["announce.dismiss","content.action.edit","content.action.view","content.code.annotate","content.code.copy","content.code.select","content.footnote.tooltips","content.tabs.link","content.tooltips","header.autohide","navigation.expand","navigation.footer","navigation.indexes","navigation.instant","navigation.instant.prefetch","navigation.instant.progress","navigation.path","navigation.prune","navigation.sections","navigation.tabs","navigation.tabs.sticky","navigation.top","navigation.tracking","search.highlight","toc.follow"],"search":"../../../assets/javascripts/workers/search.e2d2d235.min.js","tags":null,"translations":{"clipboard.copied":"已复制","clipboard.copy":"复制","search.result.more.one":"在该页上还有 1 个符合条件的结果","search.result.more.other":"在该页上还有 # 个符合条件的结果","search.result.none":"没有找到符合条件的结果","search.result.one":"找到 1 个符合条件的结果","search.result.other":"# 个符合条件的结果","search.result.placeholder":"键入以开始搜索","search.result.term.missing":"缺少","select.version":"选择当前版本"},"version":null}</script>
    
    
      <script src="../../../assets/javascripts/bundle.8ffeb9c9.min.js"></script>
      
        <script src="../../../javascripts/mathjax.js"></script>
      
        <script src="https://unpkg.com/mathjax@3/es5/tex-mml-chtml.js"></script>
      
    
  </body>
</html>